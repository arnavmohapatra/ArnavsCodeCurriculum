{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Tensorflow handles the Training and Testing\n",
    "from tensorflow import keras #Keras handles the importing of Data\n",
    "import numpy as np #NumPy does funny math good\n",
    "import gym #imports OpenAI Gym which has a bunch of environments(games) to play with\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median \n",
    "from tqdm import tqdm\n",
    "from keras.models import load_model\n",
    "from keras.activations import relu, linear\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mean_squared_error\n",
    "import random\n",
    "from collections import deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\nBox(0, 255, (96, 96, 3), uint8)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v0\")\n",
    "print(env.action_space.shape[0])\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Runs 5 games with bottom thruster firing \n",
    "for _ in range(10000):\n",
    "    env.reset()\n",
    "    for s in range(300):\n",
    "        \n",
    "        env.render(True) #Renders Environment. CAUTION: Rendering takes more time to train\n",
    "        \n",
    "        #Picks Action based on max reward\n",
    "        action = 0\n",
    "        if s%3==0: #Fires every 3rd frame\n",
    "            action = 2\n",
    "        \n",
    "        new_state, reward, done, _ = env.step(action) #Takes the action\n",
    "\n",
    "        #changes states\n",
    "        state = new_state\n",
    "\n",
    "        # #Handles if game finished\n",
    "        # if done:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the DQN\n",
    "class DQN():\n",
    "    def __init__(self, env, lr, gamma, epsilon, epsilon_decay):\n",
    "        # Hyperparameters\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "        # Environment variables\n",
    "        self.env = env\n",
    "        self.action_space = env.action_space\n",
    "        self.observation_space = env.observation_space\n",
    "        self.num_action_space = env.action_space.n\n",
    "        self.num_observation_space = env.observation_space.shape[0]\n",
    "\n",
    "        # Training Variables \n",
    "        self.training_data = deque(maxlen=500000)\n",
    "        self.rewards_list = []\n",
    "        self.batch_size = 64\n",
    "        self.high_score = -8000\n",
    "\n",
    "        # Creating DQN with Architecture 512-256-4\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(512, input_dim=self.num_observation_space, activation=relu))\n",
    "        model.add(keras.layers.Dense(256, activation=relu))\n",
    "        model.add(keras.layers.Dense(self.num_action_space, activation=linear))\n",
    "\n",
    "        # Compiling Model using MSE Loss and Adam Optimizer\n",
    "        model.compile(loss=mean_squared_error, optimizer=Adam(lr=self.lr))\n",
    "\n",
    "        self.model = model\n",
    "        print(model.summary())\n",
    "    \n",
    "    # Chooses an action based on the Epsilon value (Random action Epsilon% of the time)\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.num_action_space)\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "\n",
    "    # Trains model based off of Cumulative Training Data\n",
    "    def learn(self): #COME BACK HERE AND FIND OUT WTF HAPPENED\n",
    "\n",
    "        # Cancels Training if there is insufficient data or if ther model is sufficiently trained\n",
    "        if len(self.training_data) < self.batch_size:\n",
    "            return\n",
    "        if np.mean(self.rewards_list[-10:]) > 180:\n",
    "            return\n",
    "\n",
    "        # Randomly Samples frames out of Training Data based on self.batch_size\n",
    "        sample = random.sample(self.training_data, self.batch_size)\n",
    "        \n",
    "        # Extracts components from each frame and condenses them into arrays\n",
    "        states = np.squeeze(np.squeeze(np.array([i[0] for i in sample])))\n",
    "        actions = np.array([i[1] for i in sample])\n",
    "        rewards = np.array([i[2] for i in sample])\n",
    "        new_states = np.squeeze(np.array([i[3] for i in sample]))\n",
    "        done_list = np.array([i[4] for i in sample])\n",
    "        \n",
    "        # Creates \"targets\" for model.fit()\n",
    "        targets = rewards + self.gamma * (np.amax(self.model.predict_on_batch(new_states), axis=1)) * (1 - done_list)\n",
    "        target_vec = self.model.predict_on_batch(states)\n",
    "        indexes = np.array([i for i in range(len(self.batch_size))])\n",
    "        target_vec[[indexes], [actions]] = targets\n",
    "        \n",
    "        self.model.fit(states, target_vec, epochs=1, verbose=0)\n",
    "\n",
    "    # Handles Generating Training Episodes and Trains Model\n",
    "    def train(self, episodes = 500):\n",
    "        progress = tqdm(total=episodes, position=0, leave=False)\n",
    "        \n",
    "        # Epsiodes Loop\n",
    "        for e in range(episodes):\n",
    "            progress.update(1)\n",
    "\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            MAX_STEPS = 1000\n",
    "            state = np.reshape(state, [1, self.num_observation_space])\n",
    "\n",
    "            # Step Loop\n",
    "            for s in range(MAX_STEPS):\n",
    "                #env.render()\n",
    "\n",
    "                action = self.get_action(state) # Chooses action\n",
    "\n",
    "                new_state, reward, done, _ = env.step(action) # Takes Action and records New State\n",
    "                new_state = np.reshape(new_state, [1, self.num_observation_space])\n",
    "\n",
    "                self.training_data.append((state, action, reward, new_state, done)) # adds information about the fram to training data\n",
    "                \n",
    "                episode_reward += reward # Reward tally\n",
    "\n",
    "                state = new_state #Progressing of game\n",
    "                \n",
    "                self.learn() \n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            self.rewards_list.append(episode_reward) # Tracks rewards and keeps a high score\n",
    "            if self.high_score < episode_reward:\n",
    "                self.high_score = episode_reward\n",
    "\n",
    "            if self.epsilon > self.epsilon_min: # Handles epsilon decay over the course of the episode\n",
    "                self.epsilon *= self.epsilon_decay #(episodes-e)/episodes \n",
    "            \n",
    "            if np.mean(self.rewards_list[-100:]) > 200: # Stops training if Scores are above 200\n",
    "                print(\"Average Score: 200. Training Completed...\")\n",
    "                break\n",
    "            \n",
    "            \n",
    "    \n",
    "            print(\" || Reward: \", \"%.2f\" % episode_reward, \"\\t|| Average Reward: \", \"%.2f\" % np.mean(self.rewards_list[-100:]), \"\\t epsilon: \", \"%.4f\" % self.epsilon )\n",
    "\n",
    "        print(\"Training Complete...\")\n",
    "        print(\"Highest Training Score:\", self.high_score)\n",
    "\n",
    "    # Saves Model in .h5 format\n",
    "    def save(self, name):\n",
    "        self.model.save(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}