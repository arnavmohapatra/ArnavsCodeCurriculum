{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitcf688b4200924e1a9de8aa02cb2c3f5c",
   "display_name": "Python 3.8.2 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #Tensorflow handles the Training and Testing\n",
    "from tensorflow import keras #Keras handles the importing of Data\n",
    "import numpy as np #NumPy does funny math good\n",
    "import gym #imports OpenAI Gym which has a bunch of environments(games) to play with\n",
    "import matplotlib.pyplot as plt\n",
    "from statistics import mean, median\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment Notes:\n",
    "#   -new_state is an array of 4 values \n",
    "#       Num\t    Observation\t            Min\t        Max\n",
    "#       0\t    Cart Position\t        -2.4\t    2.4\n",
    "#       1\t    Cart Velocity\t        -Inf\t    Inf\n",
    "#       2\t    Pole Angle\t            ~ -41.8°\t~ 41.8°\n",
    "#       3\t    Pole Velocity At Tip\t-Inf\t    Inf\n",
    "# \n",
    "#   - actions are left or right mapped as 0=left 1=right\n",
    "# \n",
    "#   - Episodes are terminated if:\n",
    "#       - Pole Angle is more than ±12°\n",
    "#       - Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "#       - Episode length is greater than 200 (500 for v1).\n",
    "# \n",
    "#   - Problem is considered solved if:\n",
    "#       - Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Action Test\n",
    "# for e in range(10):\n",
    "#     state = env.reset()\n",
    "#     for s in range(100):\n",
    "\n",
    "#         env.render()\n",
    "#         action = env.action_space.sample()\n",
    "#         new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "#         # if done:\n",
    "#         #     break\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Data Generation Progress: 100%|█████████▉| 27497/27500 [00:07<00:00, 3928.67it/s]\n",
      "Training data has 1065 episodes (Loss = 96.12727272727273 %)\n",
      "Average accepted score: 61.3962441314554\n",
      "Median score for accepted scores: 58.0\n",
      "Highest Score:  28\n",
      "Data Generation Progress: 100%|██████████| 27500/27500 [00:22<00:00, 3928.67it/s]"
     ]
    }
   ],
   "source": [
    "#Hpyer Parameters\n",
    "score_threshhold = 50\n",
    "n_training_games = 27500\n",
    "n_training_steps = 500\n",
    "\n",
    "#Creating Training Data\n",
    "training_data = [] #Each Piece of data will be in format (observations, moves)\n",
    "\n",
    "loss = 0\n",
    "progress = tqdm(total=n_training_games, position=0, leave=False)\n",
    "progress.set_description(\"Data Generation Progress\")\n",
    "scores = [] #List of good data\n",
    "for _ in range(n_training_games): #Generate 20000 training data\n",
    "    score = 0 #Score counter over episode\n",
    "\n",
    "    memory = [] #List of all moves\n",
    "    prev_obs = [] #Last observation made\n",
    "\n",
    "    progress.update(1)\n",
    "\n",
    "    env.reset()\n",
    "    for _ in range(n_training_steps): #Runs 250 steps per episode\n",
    "        action = env.action_space.sample() #Chooses random action\n",
    "        new_state, reward, done, _ = env.step(action) #Runs random action\n",
    "\n",
    "        if(len(prev_obs)>0): #Except for the first time, add all sets of previous observations and current actions to memory\n",
    "            memory.append([prev_obs, action])\n",
    "        prev_obs = new_state #Update previous observation\n",
    "\n",
    "        score+=reward #Update Score\n",
    "        if done: break #Episode data is <= 250 observations \n",
    "\n",
    "    if score >= score_threshhold:  \n",
    "        scores.append(score)\n",
    "        \n",
    "        #Converting actions(left,right) to one-hot(0,1) representation\n",
    "        #NOTE: data[0] is the observation of size 4, data[1] is resulting left-right action\n",
    "        episode_data = []\n",
    "        for data in memory:\n",
    "            if(data[1] == 0):\n",
    "                data[1]=[1,0]\n",
    "            elif(data[1] == 1):\n",
    "                data[1]=[0,1]\n",
    "                \n",
    "            episode_data.append(data) #Compiles data for training set\n",
    "        training_data.append(episode_data)\n",
    "    else: loss+=1\n",
    "\n",
    "print()\n",
    "print(\"Training data has\", len(training_data), \"episodes (Loss =\", (loss/n_training_games)*100, \"%)\")\n",
    "print('Average accepted score:', mean(scores))\n",
    "print('Median score for accepted scores:', median(scores))\n",
    "print(\"Highest Score: \", np.argmax(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'units'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-555e3235bb0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m model = tf.keras.Sequential([\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"input_layer\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"dense_1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'units'"
     ]
    }
   ],
   "source": [
    "#Making the Model\n",
    "alpha = .01 #Learning Rate\n",
    "alpha_decay = .01\n",
    "epsilon = 1.0 #Exploration\n",
    "epsilon_min = 0.1\n",
    "epsilon_decay = 0.995\n",
    "gamma = 1.0 #Discount Factor\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    keras.layers.Dense(input_shape=4, name = \"input_layer\"),\n",
    "\n",
    "    keras.layers.Dense(128, activation='relu', name=\"dense_1\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(256, activation='relu', name=\"dense_2\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(512, activation='relu', name=\"dense_3\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(256, activation='relu', name=\"dense_4\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(128, activation='relu', name=\"dense_5\"),\n",
    "    keras.layers.Dropout(0.8),\n",
    "\n",
    "    keras.layers.Dense(2, activation='softmax', name=\"output\"),\n",
    "])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = alpha, decay = alpha_decay, ), loss =\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-977ff3655261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain_moves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_moves\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#Training Model\n",
    "\n",
    "#Splitting training_data in x and y\n",
    "#converts states from [cart_pos, cart_vel, pole_pos, pole,vel] to [[cart_pos], [cart_vel], [pole_pos], [pole_vel]]\n",
    "train_states = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
    "train_moves = np.array([i[1] for i in training_data])\n",
    "\n",
    "model.fit(train_states, train_moves, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Learning\n",
    "# EPISODES = 150000\n",
    "# MAX_STEPS = 1000\n",
    "# EPSILON = 0.9\n",
    "# from tqdm import tqdm\n",
    "# progress = tqdm(total=EPISODES, position=0, leave=False)\n",
    "# progress.set_description(\"Progress\")\n",
    "\n",
    "# #Q-Table\n",
    "# ALPHA = 0.81 #Represents the Learning Rate. This determines how much the agent will explore. High LR means more exploration\n",
    "# GAMMA = 0.96 #Represents the Discount Factor. This determines how much the agent values the future reward. High DF means future rewards are more heavily considered\n",
    "\n",
    "# rewards = [] #Log of Rewards per Episode\n",
    "\n",
    "# #Training\n",
    "# for e in range(EPISODES):\n",
    "#     progress.update(1)\n",
    "    \n",
    "#     state = env.reset() #Resets Environment\n",
    "\n",
    "#     for s in range(MAX_STEPS):\n",
    "        \n",
    "#         #env.render() #Renders Environment. CAUTION: Rendering takes more time to train\n",
    "        \n",
    "#         #Picks Action. Chooses a random action EPSILON% of the time. Otherwise chooses the max reward option\n",
    "#         if(np.random.uniform(0,1) < EPSILON):\n",
    "#             action = env.action_space.sample() #Takes a random action, samples from env.action_space which is a list of possible actions {0: Left, 1: Down, 2: Right, 3: Up}\n",
    "#         else: \n",
    "#             action = np.argmax(Q[state, : ])\n",
    "#         new_state, reward, done, _ = env.step(action) #Takes the action\n",
    "\n",
    "#         #Updating Q-Table\n",
    "#         Q[state, action] = getQ(state, action)\n",
    "\n",
    "#         #changes states\n",
    "#         state = new_state\n",
    "\n",
    "#         #Handles if game finished\n",
    "#         if done:\n",
    "#             rewards.append(reward) #Adds \n",
    "#             EPSILON -= 1/EPISODES #Steps down the random action rate to prioritize rewards over exploration\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Plotting the learning\n",
    "# def get_average(values):\n",
    "#   return sum(values)/len(values)\n",
    "\n",
    "# avg_rewards = []\n",
    "# for i in range(0, len(rewards), 100):\n",
    "#   avg_rewards.append(get_average(rewards[i:i+100])) \n",
    "\n",
    "# plt.plot(avg_rewards)\n",
    "# plt.ylabel('average reward')\n",
    "# plt.xlabel('episodes (100\\'s)')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.reset() #Resets Environment\n",
    "\n",
    "# for s in range(MAX_STEPS):\n",
    "    \n",
    "#     env.render() #Renders Environment. CAUTION: Rendering takes more time to train\n",
    "    \n",
    "#     #Picks Action based on max reward\n",
    "#     action = np.argmax(Q[state, : ])\n",
    "#     new_state, reward, done, _ = env.step(action) #Takes the action\n",
    "\n",
    "#     #changes states\n",
    "#     state = new_state\n",
    "\n",
    "#     #Handles if game finished\n",
    "#     if done:\n",
    "#         break\n",
    "# print(\"It took\", s, \"steps to finish\")\n",
    "# env.render()"
   ]
  }
 ]
}